{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this analysis is to examine the efficacy of college standardized admissions tests and whether they're unfair to particular demographics. As such, we will be investigating the potential correlation between SAT/AP test scores and various demographic factors, such as race, gender, and income. \n",
    "\n",
    "Data was taken from the NYC Open Data website concerning NYC high schools. The files are as follows:\n",
    "\n",
    "    * 'ap_scores' - Data on AP test scores\n",
    "    * 'class_size' - Data on class size\n",
    "    * 'demographics' - Data on class high school demographics\n",
    "    * 'graduation_outcomes' - Data on graduation rates, etc.\n",
    "    * 'hs_directory' - A general directory of the high schools\n",
    "    * 'sat_results' - Data on SAT scores\n",
    "    * 'all_survey' - Data on surveys from all NYC high schools\n",
    "    * 'd75_survey' - Data on surveys from NYC district 75\n",
    "   \n",
    "Currently, this file is an exploration in data cleaning. Analysis of the data is ongoing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ap_scores.csv', 'class_size.csv', 'demographics.csv', 'graduation_outcomes.csv', 'hs_directory.csv', 'sat_results.csv']\n",
      "Index(['DBN', 'SchoolName', 'AP Test Takers ', 'Total Exams Taken',\n",
      "       'Number of Exams with scores 3 4 or 5'],\n",
      "      dtype='object')\n",
      "Index(['CSD', 'BOROUGH', 'SCHOOL CODE', 'SCHOOL NAME', 'GRADE ',\n",
      "       'PROGRAM TYPE', 'CORE SUBJECT (MS CORE and 9-12 ONLY)',\n",
      "       'CORE COURSE (MS CORE and 9-12 ONLY)', 'SERVICE CATEGORY(K-9* ONLY)',\n",
      "       'NUMBER OF STUDENTS / SEATS FILLED', 'NUMBER OF SECTIONS',\n",
      "       'AVERAGE CLASS SIZE', 'SIZE OF SMALLEST CLASS', 'SIZE OF LARGEST CLASS',\n",
      "       'DATA SOURCE', 'SCHOOLWIDE PUPIL-TEACHER RATIO'],\n",
      "      dtype='object')\n",
      "Index(['DBN', 'Name', 'schoolyear', 'fl_percent', 'frl_percent',\n",
      "       'total_enrollment', 'prek', 'k', 'grade1', 'grade2', 'grade3', 'grade4',\n",
      "       'grade5', 'grade6', 'grade7', 'grade8', 'grade9', 'grade10', 'grade11',\n",
      "       'grade12', 'ell_num', 'ell_percent', 'sped_num', 'sped_percent',\n",
      "       'ctt_num', 'selfcontained_num', 'asian_num', 'asian_per', 'black_num',\n",
      "       'black_per', 'hispanic_num', 'hispanic_per', 'white_num', 'white_per',\n",
      "       'male_num', 'male_per', 'female_num', 'female_per'],\n",
      "      dtype='object')\n",
      "Index(['Demographic', 'DBN', 'School Name', 'Cohort', 'Total Cohort',\n",
      "       'Total Grads - n', 'Total Grads - % of cohort', 'Total Regents - n',\n",
      "       'Total Regents - % of cohort', 'Total Regents - % of grads',\n",
      "       'Advanced Regents - n', 'Advanced Regents - % of cohort',\n",
      "       'Advanced Regents - % of grads', 'Regents w/o Advanced - n',\n",
      "       'Regents w/o Advanced - % of cohort',\n",
      "       'Regents w/o Advanced - % of grads', 'Local - n', 'Local - % of cohort',\n",
      "       'Local - % of grads', 'Still Enrolled - n',\n",
      "       'Still Enrolled - % of cohort', 'Dropped Out - n',\n",
      "       'Dropped Out - % of cohort'],\n",
      "      dtype='object')\n",
      "Index(['dbn', 'school_name', 'borough', 'building_code', 'phone_number',\n",
      "       'fax_number', 'grade_span_min', 'grade_span_max', 'expgrade_span_min',\n",
      "       'expgrade_span_max', 'bus', 'subway', 'primary_address_line_1', 'city',\n",
      "       'state_code', 'postcode', 'website', 'total_students', 'campus_name',\n",
      "       'school_type', 'overview_paragraph', 'program_highlights',\n",
      "       'language_classes', 'advancedplacement_courses', 'online_ap_courses',\n",
      "       'online_language_courses', 'extracurricular_activities',\n",
      "       'psal_sports_boys', 'psal_sports_girls', 'psal_sports_coed',\n",
      "       'school_sports', 'partner_cbo', 'partner_hospital', 'partner_highered',\n",
      "       'partner_cultural', 'partner_nonprofit', 'partner_corporate',\n",
      "       'partner_financial', 'partner_other', 'addtl_info1', 'addtl_info2',\n",
      "       'start_time', 'end_time', 'se_services', 'ell_programs',\n",
      "       'school_accessibility_description', 'number_programs', 'priority01',\n",
      "       'priority02', 'priority03', 'priority04', 'priority05', 'priority06',\n",
      "       'priority07', 'priority08', 'priority09', 'priority10', 'Location 1',\n",
      "       'Community Board', 'Council District', 'Census Tract', 'BIN', 'BBL',\n",
      "       'NTA'],\n",
      "      dtype='object')\n",
      "Index(['DBN', 'SCHOOL NAME', 'Num of SAT Test Takers',\n",
      "       'SAT Critical Reading Avg. Score', 'SAT Math Avg. Score',\n",
      "       'SAT Writing Avg. Score'],\n",
      "      dtype='object')\n",
      "DBN                                                           01M448\n",
      "SchoolName                              UNIVERSITY NEIGHBORHOOD H.S.\n",
      "AP Test Takers                                                    39\n",
      "Total Exams Taken                                                 49\n",
      "Number of Exams with scores 3 4 or 5                              10\n",
      "Name: 0, dtype: object\n",
      "CSD                                                             1\n",
      "BOROUGH                                                         M\n",
      "SCHOOL CODE                                                  M015\n",
      "SCHOOL NAME                             P.S. 015 Roberto Clemente\n",
      "GRADE                                                          0K\n",
      "PROGRAM TYPE                                               GEN ED\n",
      "CORE SUBJECT (MS CORE and 9-12 ONLY)                            -\n",
      "CORE COURSE (MS CORE and 9-12 ONLY)                             -\n",
      "SERVICE CATEGORY(K-9* ONLY)                                     -\n",
      "NUMBER OF STUDENTS / SEATS FILLED                              19\n",
      "NUMBER OF SECTIONS                                              1\n",
      "AVERAGE CLASS SIZE                                             19\n",
      "SIZE OF SMALLEST CLASS                                         19\n",
      "SIZE OF LARGEST CLASS                                          19\n",
      "DATA SOURCE                                                   ATS\n",
      "SCHOOLWIDE PUPIL-TEACHER RATIO                                NaN\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data ={}\n",
    "\n",
    "## We'll read in the .csv files first. Survey results will be read in during the following step using a different pandas tool.\n",
    "file_list = ['ap_scores.csv','hs_directory.csv','class_size.csv','demographics.csv','graduation_outcomes.csv','sat_results.csv']\n",
    "print(sorted(file_list))\n",
    "for file_name in sorted(file_list):\n",
    "    string_name = str.replace(file_name,'.csv','')\n",
    "    data[string_name] = pd.read_csv(file_name)\n",
    "    print(data[string_name].columns)\n",
    "    \n",
    "print(data['ap_scores'].loc[0,:])\n",
    "print(data['class_size'].loc[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, each of the data sets appear to have a 'DBN' column (with the exception of 'class_size' and 'hs_directory'), which provides a unique identifer for each of the schools. We can use this as we clean and merge the data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now that we've read in our .csv files, we can read in the two survey files, which are .xlxs files. We should also merge them\n",
    "# together to save time as we add them to our 'data' dictionary. \n",
    "\n",
    "all_survey = pd.io.excel.read_excel('all_survey.xlsx')\n",
    "d75_survey = pd.io.excel.read_excel('d75_survey.xlsx')\n",
    "survey = pd.concat([all_survey,d75_survey],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1702, 23)\n",
      "      DBN  rr_s  rr_t  rr_p  N_s   N_t   N_p  saf_p_11  com_p_11  eng_p_11  \\\n",
      "0  01M015   NaN    88    60  NaN  22.0  90.0       8.5       7.6       7.5   \n",
      "\n",
      "      ...      eng_t_11  aca_t_11  saf_s_11  com_s_11  eng_s_11  aca_s_11  \\\n",
      "0     ...           7.6       7.9       NaN       NaN       NaN       NaN   \n",
      "\n",
      "   saf_tot_11  com_tot_11  eng_tot_11  aca_tot_11  \n",
      "0         8.0         7.7         7.5         7.9  \n",
      "\n",
      "[1 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# The survey data contains far more columns of information than we need for our analysis. Rather than adding the entire data set\n",
    "# into our dictionary, we can examine the data dictionary that accompanied the data sets to see what is most useful to us. After\n",
    "# a look at the dictionary, it seems as though the following columns are most relevant.\n",
    "columns = [\"DBN\", \"rr_s\", \"rr_t\", \"rr_p\", \"N_s\", \"N_t\", \"N_p\", \"saf_p_11\", \"com_p_11\", \"eng_p_11\", \"aca_p_11\", \"saf_t_11\", \"com_t_11\", \"eng_t_11\", \"aca_t_11\", \"saf_s_11\", \"com_s_11\", \"eng_s_11\", \"aca_s_11\", \"saf_tot_11\", \"com_tot_11\", \"eng_tot_11\", \"aca_tot_11\"]\n",
    "survey['DBN'] = survey['dbn']\n",
    "\n",
    "survey = survey[columns]\n",
    "data['survey'] = survey\n",
    "print(data['survey'].shape)\n",
    "print(data['survey'].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    01M015\n",
       "1    01M015\n",
       "2    01M015\n",
       "3    01M015\n",
       "4    01M015\n",
       "Name: DBN, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are two more steps we can take to make data merging and analysis easier: (1) we can create DBN columns for class_size \n",
    "# and hs_directory, and (2) we can combine the SAT scores so we only have the aggregate SAT score. \n",
    "\n",
    "## Create 'DBN' column for hs_directory.\n",
    "data['hs_directory']['DBN'] = data['hs_directory']['dbn']\n",
    "\n",
    "## A school's DBN in class_size seems to be a combination of it's CSD and School Code, albeit with an extra '0' in front. To help\n",
    "## us complete the merge, we can use this info to create a DBN column for class_size.\n",
    "def create_dbn(csd):\n",
    "    string_csd = str(csd)\n",
    "    if len(string_csd) < 2:\n",
    "        return string_csd.zfill(2)\n",
    "    else:\n",
    "        return string_csd\n",
    "\n",
    "padded_csd = data['class_size']['CSD'].apply(create_dbn)\n",
    "data['class_size']['DBN'] = padded_csd + data['class_size']['SCHOOL CODE']\n",
    "data['class_size']['DBN'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1122.0\n",
       "1    1172.0\n",
       "2    1149.0\n",
       "3    1174.0\n",
       "4    1207.0\n",
       "Name: sat_score, dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now we can calulate total average SAT score for each school. First, we need to transform each column in integers for calculations.\n",
    "sat_sections = ['SAT Math Avg. Score','SAT Critical Reading Avg. Score', 'SAT Writing Avg. Score']\n",
    "for each in sat_sections:\n",
    "    data['sat_results'][each] = pd.to_numeric(data['sat_results'][each], errors='coerce')\n",
    "\n",
    "data['sat_results']['sat_score'] = data['sat_results']['SAT Math Avg. Score'] + data['sat_results']['SAT Critical Reading Avg. Score'] + data['sat_results']['SAT Writing Avg. Score']\n",
    "data['sat_results']['sat_score'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    883 Classon Avenue\\nBrooklyn, NY 11225\\n(40.67...\n",
      "Name: Location 1, dtype: object\n",
      "      dbn                                  school_name   borough  \\\n",
      "0  17K548          Brooklyn School for Music & Theatre  Brooklyn   \n",
      "1  09X543             High School for Violin and Dance     Bronx   \n",
      "2  09X327  Comprehensive Model School Project M.S. 327     Bronx   \n",
      "\n",
      "  building_code  phone_number    fax_number  grade_span_min  grade_span_max  \\\n",
      "0          K440  718-230-6250  718-230-6262             9.0              12   \n",
      "1          X400  718-842-0687  718-589-9849             9.0              12   \n",
      "2          X240  718-294-8111  718-294-8109             6.0              12   \n",
      "\n",
      "   expgrade_span_min  expgrade_span_max    ...      \\\n",
      "0                NaN                NaN    ...       \n",
      "1                NaN                NaN    ...       \n",
      "2                NaN                NaN    ...       \n",
      "\n",
      "                                          Location 1 Community Board  \\\n",
      "0  883 Classon Avenue\\nBrooklyn, NY 11225\\n(40.67...             9.0   \n",
      "1  1110 Boston Road\\nBronx, NY 10456\\n(40.8276026...             3.0   \n",
      "2  1501 Jerome Avenue\\nBronx, NY 10452\\n(40.84241...             4.0   \n",
      "\n",
      "  Council District Census Tract        BIN           BBL  \\\n",
      "0             35.0        213.0  3029686.0  3.011870e+09   \n",
      "1             16.0        135.0  2004526.0  2.026340e+09   \n",
      "2             14.0        209.0  2008336.0  2.028590e+09   \n",
      "\n",
      "                                                 NTA     DBN        lat  \\\n",
      "0  Crown Heights South                           ...  17K548  40.670299   \n",
      "1  Morrisania-Melrose                            ...  09X543  40.827603   \n",
      "2  West Concourse                                ...  09X327  40.842414   \n",
      "\n",
      "         lon  \n",
      "0 -73.961648  \n",
      "1 -73.904475  \n",
      "2 -73.916162  \n",
      "\n",
      "[3 rows x 67 columns]\n"
     ]
    }
   ],
   "source": [
    "## Before moving onto the merge, let's parse the geographic info from the hs_directory data set. This will allow us to geographically\n",
    "## plot the high schools if we desire. The latitude and longitude is contained within 'Location 1,' but we'll need to extract it.\n",
    "print(data['hs_directory']['Location 1'].head(1))\n",
    "import re\n",
    "def find_lat(line):\n",
    "    coordinates = re.findall('\\(.+\\)',line)\n",
    "    return coordinates[0].split(',')[0].replace('(','')\n",
    "def find_long(line):\n",
    "    coordinates = re.findall('\\(.+\\)',line)\n",
    "    return coordinates[0].split(',')[1].replace(')','')\n",
    "\n",
    "data['hs_directory']['lat'] = pd.to_numeric(data['hs_directory']['Location 1'].apply(find_lat),errors='coerce')\n",
    "data['hs_directory']['lon'] = pd.to_numeric(data['hs_directory']['Location 1'].apply(find_long),errors='coerce')\n",
    "\n",
    "print(data['hs_directory'].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've manipulated the data sets to contain the proper columns necessary for analysis, we are almost ready to proceed with the merge. However, there are still multiple data sets that contain repeat 'DBN' values; since we plan on using the 'DBN' column to merge the data sets, this will be problematic. We'll need to condense the data sets before we continue onward to minimize data loss. We'll also need to finish up some last minute preparations (e.g. transforming strings to numerical values) so that future analysis becomes easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap_scores 258 Index(['DBN', 'SchoolName', 'AP Test Takers ', 'Total Exams Taken',\n",
      "       'Number of Exams with scores 3 4 or 5'],\n",
      "      dtype='object')\n",
      "class_size 27611 Index(['CSD', 'BOROUGH', 'SCHOOL CODE', 'SCHOOL NAME', 'GRADE ',\n",
      "       'PROGRAM TYPE', 'CORE SUBJECT (MS CORE and 9-12 ONLY)',\n",
      "       'CORE COURSE (MS CORE and 9-12 ONLY)', 'SERVICE CATEGORY(K-9* ONLY)',\n",
      "       'NUMBER OF STUDENTS / SEATS FILLED', 'NUMBER OF SECTIONS',\n",
      "       'AVERAGE CLASS SIZE', 'SIZE OF SMALLEST CLASS', 'SIZE OF LARGEST CLASS',\n",
      "       'DATA SOURCE', 'SCHOOLWIDE PUPIL-TEACHER RATIO', 'DBN'],\n",
      "      dtype='object')\n",
      "demographics 10075 Index(['DBN', 'Name', 'schoolyear', 'fl_percent', 'frl_percent',\n",
      "       'total_enrollment', 'prek', 'k', 'grade1', 'grade2', 'grade3', 'grade4',\n",
      "       'grade5', 'grade6', 'grade7', 'grade8', 'grade9', 'grade10', 'grade11',\n",
      "       'grade12', 'ell_num', 'ell_percent', 'sped_num', 'sped_percent',\n",
      "       'ctt_num', 'selfcontained_num', 'asian_num', 'asian_per', 'black_num',\n",
      "       'black_per', 'hispanic_num', 'hispanic_per', 'white_num', 'white_per',\n",
      "       'male_num', 'male_per', 'female_num', 'female_per'],\n",
      "      dtype='object')\n",
      "graduation_outcomes 25096 Index(['Demographic', 'DBN', 'School Name', 'Cohort', 'Total Cohort',\n",
      "       'Total Grads - n', 'Total Grads - % of cohort', 'Total Regents - n',\n",
      "       'Total Regents - % of cohort', 'Total Regents - % of grads',\n",
      "       'Advanced Regents - n', 'Advanced Regents - % of cohort',\n",
      "       'Advanced Regents - % of grads', 'Regents w/o Advanced - n',\n",
      "       'Regents w/o Advanced - % of cohort',\n",
      "       'Regents w/o Advanced - % of grads', 'Local - n', 'Local - % of cohort',\n",
      "       'Local - % of grads', 'Still Enrolled - n',\n",
      "       'Still Enrolled - % of cohort', 'Dropped Out - n',\n",
      "       'Dropped Out - % of cohort'],\n",
      "      dtype='object')\n",
      "hs_directory 435 Index(['dbn', 'school_name', 'borough', 'building_code', 'phone_number',\n",
      "       'fax_number', 'grade_span_min', 'grade_span_max', 'expgrade_span_min',\n",
      "       'expgrade_span_max', 'bus', 'subway', 'primary_address_line_1', 'city',\n",
      "       'state_code', 'postcode', 'website', 'total_students', 'campus_name',\n",
      "       'school_type', 'overview_paragraph', 'program_highlights',\n",
      "       'language_classes', 'advancedplacement_courses', 'online_ap_courses',\n",
      "       'online_language_courses', 'extracurricular_activities',\n",
      "       'psal_sports_boys', 'psal_sports_girls', 'psal_sports_coed',\n",
      "       'school_sports', 'partner_cbo', 'partner_hospital', 'partner_highered',\n",
      "       'partner_cultural', 'partner_nonprofit', 'partner_corporate',\n",
      "       'partner_financial', 'partner_other', 'addtl_info1', 'addtl_info2',\n",
      "       'start_time', 'end_time', 'se_services', 'ell_programs',\n",
      "       'school_accessibility_description', 'number_programs', 'priority01',\n",
      "       'priority02', 'priority03', 'priority04', 'priority05', 'priority06',\n",
      "       'priority07', 'priority08', 'priority09', 'priority10', 'Location 1',\n",
      "       'Community Board', 'Council District', 'Census Tract', 'BIN', 'BBL',\n",
      "       'NTA', 'DBN', 'lat', 'lon'],\n",
      "      dtype='object')\n",
      "sat_results 478 Index(['DBN', 'SCHOOL NAME', 'Num of SAT Test Takers',\n",
      "       'SAT Critical Reading Avg. Score', 'SAT Math Avg. Score',\n",
      "       'SAT Writing Avg. Score', 'sat_score'],\n",
      "      dtype='object')\n",
      "survey 1702 Index(['DBN', 'rr_s', 'rr_t', 'rr_p', 'N_s', 'N_t', 'N_p', 'saf_p_11',\n",
      "       'com_p_11', 'eng_p_11', 'aca_p_11', 'saf_t_11', 'com_t_11', 'eng_t_11',\n",
      "       'aca_t_11', 'saf_s_11', 'com_s_11', 'eng_s_11', 'aca_s_11',\n",
      "       'saf_tot_11', 'com_tot_11', 'eng_tot_11', 'aca_tot_11'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## First, let's examine any discrepancies across the number of rows between the data sets.\n",
    "for each in data:\n",
    "    print(each, data[each].shape[0],data[each].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0K' '01' '02' '03' '04' '05' '0K-09' nan '06' '07' '08' 'MS Core' '09-12'\n",
      " '09']\n",
      "['GEN ED' 'CTT' 'SPEC ED' nan 'G&T']\n",
      "      DBN  CSD  NUMBER OF STUDENTS / SEATS FILLED  NUMBER OF SECTIONS  \\\n",
      "0  01M292    1                            88.0000            4.000000   \n",
      "1  01M332    1                            46.0000            2.000000   \n",
      "2  01M378    1                            33.0000            1.000000   \n",
      "3  01M448    1                           105.6875            4.750000   \n",
      "4  01M450    1                            57.6000            2.733333   \n",
      "\n",
      "   AVERAGE CLASS SIZE  SIZE OF SMALLEST CLASS  SIZE OF LARGEST CLASS  \\\n",
      "0           22.564286                   18.50              26.571429   \n",
      "1           22.000000                   21.00              23.500000   \n",
      "2           33.000000                   33.00              33.000000   \n",
      "3           22.231250                   18.25              27.062500   \n",
      "4           21.200000                   19.40              22.866667   \n",
      "\n",
      "   SCHOOLWIDE PUPIL-TEACHER RATIO  \n",
      "0                             NaN  \n",
      "1                             NaN  \n",
      "2                             NaN  \n",
      "3                             NaN  \n",
      "4                             NaN  \n"
     ]
    }
   ],
   "source": [
    "## Condense class_size by filtering for high school grades in 'GRADE' only. We can also filter 'PROGRAM TYPE' down to 'GEN ED'\n",
    "## because it's the largest program type by far.\n",
    "print(data['class_size']['GRADE '].unique())\n",
    "print(data['class_size']['PROGRAM TYPE'].unique())\n",
    "class_size = data['class_size']\n",
    "class_size = class_size[class_size['GRADE '] == '09-12']\n",
    "class_size = class_size[class_size['PROGRAM TYPE'] == 'GEN ED']\n",
    "data['class_size'] = class_size\n",
    "\n",
    "## Since there still replicate DBNs, we will need to filter class_size down even further. Since a single DBN appears to correspond\n",
    "## with multiple core subjects, we need to group the data set by DBN and aggregate the data in this column by average.\n",
    "class_size = class_size.groupby('DBN').aggregate(np.mean)\n",
    "class_size.reset_index(inplace=True)\n",
    "data['class_size'] = class_size\n",
    "print(data['class_size'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DBN</th>\n",
       "      <th>Name</th>\n",
       "      <th>schoolyear</th>\n",
       "      <th>fl_percent</th>\n",
       "      <th>frl_percent</th>\n",
       "      <th>total_enrollment</th>\n",
       "      <th>prek</th>\n",
       "      <th>k</th>\n",
       "      <th>grade1</th>\n",
       "      <th>grade2</th>\n",
       "      <th>...</th>\n",
       "      <th>black_num</th>\n",
       "      <th>black_per</th>\n",
       "      <th>hispanic_num</th>\n",
       "      <th>hispanic_per</th>\n",
       "      <th>white_num</th>\n",
       "      <th>white_per</th>\n",
       "      <th>male_num</th>\n",
       "      <th>male_per</th>\n",
       "      <th>female_num</th>\n",
       "      <th>female_per</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>01M015</td>\n",
       "      <td>P.S. 015 ROBERTO CLEMENTE</td>\n",
       "      <td>20112012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89.4</td>\n",
       "      <td>189</td>\n",
       "      <td>13</td>\n",
       "      <td>31</td>\n",
       "      <td>35</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>63</td>\n",
       "      <td>33.3</td>\n",
       "      <td>109</td>\n",
       "      <td>57.7</td>\n",
       "      <td>4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>97.0</td>\n",
       "      <td>51.3</td>\n",
       "      <td>92.0</td>\n",
       "      <td>48.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>01M019</td>\n",
       "      <td>P.S. 019 ASHER LEVY</td>\n",
       "      <td>20112012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.5</td>\n",
       "      <td>328</td>\n",
       "      <td>32</td>\n",
       "      <td>46</td>\n",
       "      <td>52</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>24.7</td>\n",
       "      <td>158</td>\n",
       "      <td>48.2</td>\n",
       "      <td>28</td>\n",
       "      <td>8.5</td>\n",
       "      <td>147.0</td>\n",
       "      <td>44.8</td>\n",
       "      <td>181.0</td>\n",
       "      <td>55.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>01M020</td>\n",
       "      <td>PS 020 ANNA SILVER</td>\n",
       "      <td>20112012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.5</td>\n",
       "      <td>626</td>\n",
       "      <td>52</td>\n",
       "      <td>102</td>\n",
       "      <td>121</td>\n",
       "      <td>87</td>\n",
       "      <td>...</td>\n",
       "      <td>55</td>\n",
       "      <td>8.8</td>\n",
       "      <td>357</td>\n",
       "      <td>57.0</td>\n",
       "      <td>16</td>\n",
       "      <td>2.6</td>\n",
       "      <td>330.0</td>\n",
       "      <td>52.7</td>\n",
       "      <td>296.0</td>\n",
       "      <td>47.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>01M034</td>\n",
       "      <td>PS 034 FRANKLIN D ROOSEVELT</td>\n",
       "      <td>20112012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.7</td>\n",
       "      <td>401</td>\n",
       "      <td>14</td>\n",
       "      <td>34</td>\n",
       "      <td>38</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>22.4</td>\n",
       "      <td>275</td>\n",
       "      <td>68.6</td>\n",
       "      <td>8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>50.9</td>\n",
       "      <td>197.0</td>\n",
       "      <td>49.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>01M063</td>\n",
       "      <td>PS 063 WILLIAM MCKINLEY</td>\n",
       "      <td>20112012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.9</td>\n",
       "      <td>176</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>41</td>\n",
       "      <td>23.3</td>\n",
       "      <td>110</td>\n",
       "      <td>62.5</td>\n",
       "      <td>15</td>\n",
       "      <td>8.5</td>\n",
       "      <td>97.0</td>\n",
       "      <td>55.1</td>\n",
       "      <td>79.0</td>\n",
       "      <td>44.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       DBN                                              Name  schoolyear  \\\n",
       "6   01M015  P.S. 015 ROBERTO CLEMENTE                           20112012   \n",
       "13  01M019  P.S. 019 ASHER LEVY                                 20112012   \n",
       "20  01M020  PS 020 ANNA SILVER                                  20112012   \n",
       "27  01M034  PS 034 FRANKLIN D ROOSEVELT                         20112012   \n",
       "35  01M063  PS 063 WILLIAM MCKINLEY                             20112012   \n",
       "\n",
       "   fl_percent  frl_percent  total_enrollment prek    k grade1 grade2  \\\n",
       "6         NaN         89.4               189   13   31     35     28   \n",
       "13        NaN         61.5               328   32   46     52     54   \n",
       "20        NaN         92.5               626   52  102    121     87   \n",
       "27        NaN         99.7               401   14   34     38     36   \n",
       "35        NaN         78.9               176   18   20     30     21   \n",
       "\n",
       "      ...     black_num black_per hispanic_num hispanic_per white_num  \\\n",
       "6     ...            63      33.3          109         57.7         4   \n",
       "13    ...            81      24.7          158         48.2        28   \n",
       "20    ...            55       8.8          357         57.0        16   \n",
       "27    ...            90      22.4          275         68.6         8   \n",
       "35    ...            41      23.3          110         62.5        15   \n",
       "\n",
       "   white_per male_num male_per female_num female_per  \n",
       "6        2.1     97.0     51.3       92.0       48.7  \n",
       "13       8.5    147.0     44.8      181.0       55.2  \n",
       "20       2.6    330.0     52.7      296.0       47.3  \n",
       "27       2.0    204.0     50.9      197.0       49.1  \n",
       "35       8.5     97.0     55.1       79.0       44.9  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now we can condense demographics. Since the only thing that's duplicating DBNs in this\n",
    "## data set is 'schoolyear', we can select only the most recent school year (2011-2012) for filtration.\n",
    "data['demographics'] = data['demographics'][data['demographics']['schoolyear'] == 20112012]\n",
    "data['demographics'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally, we can condense the graduation data set. In this data set, it's the 'Demographic' and 'Cohort' columns causing \n",
    "## replicate DBNs to appear.\n",
    "data['graduation_outcomes']['Demographic'].unique,data['graduation_outcomes']['Cohort'].unique\n",
    "## It appears that we want to include only columns with 'Total Cohort' for 'Demographic' and '2006' (the most recent cohort)\n",
    "## for 'Cohort.'\n",
    "data['graduation_outcomes'] = data['graduation_outcomes'][data['graduation_outcomes']['Demographic'] == 'Total Cohort']\n",
    "data['graduation_outcomes'] = data['graduation_outcomes'][data['graduation_outcomes']['Cohort'] == '2006']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before merging the data sets, let's take a second to convert the strings in ap_scores to numeric values for easier analysis.\n",
    "data['ap_scores'].columns\n",
    "columns_to_convert = ['AP Test Takers ','Total Exams Taken','Number of Exams with scores 3 4 or 5']\n",
    "for column in columns_to_convert:\n",
    "    data['ap_scores'][column] = pd.to_numeric(data['ap_scores'][column],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sat_results (478, 7)\n",
      "ap_scores (258, 5)\n",
      "graduation_outcomes (405, 23)\n",
      "class_size (583, 8)\n",
      "demographics (1509, 38)\n",
      "survey (1702, 23)\n",
      "hs_directory (435, 67)\n"
     ]
    }
   ],
   "source": [
    "dataframes = ['sat_results','ap_scores','graduation_outcomes','class_size','demographics','survey','hs_directory']\n",
    "\n",
    "for each in dataframes:\n",
    "    print(each,data[each].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've reasonably condensed each data set and finished preparations, we can finally continue on with the merge. Because our primary question concerns the correlation between test scores and demographics, we'll want to follow these merge strategies:\n",
    " \n",
    "    * Because sat_results is the data set we're primarily interested in, we'll use it as our starter data set for the merge.\n",
    "    * Because ap_scores and graduation_outcomes have missing DBN values in relation to sat_results, we'll perform a left \n",
    "      merge. This allows us to keep the DBN values from sat_results while adding info from these data sets.\n",
    "    * Because the other data sets, class_size, demographics, survey, and hs_directory, all contain valuable information that   we don't want to lose, we'll perform an inner merge.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(479, 33)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = data['sat_results']\n",
    "combined = combined.merge(data['ap_scores'],how='left',on='DBN')\n",
    "combined = combined.merge(data['graduation_outcomes'],how='left',on='DBN')\n",
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363, 297)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_merge = ['class_size', 'demographics','survey','hs_directory']\n",
    "for df in to_merge:\n",
    "    combined = combined.merge(data[df],how='inner',on='DBN')\n",
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Now that we've merged all the data sets, we can address rows with missing data from the merge using the df.fillna() method. \n",
    "## To simplify things, let's go ahead and just fill in the missing values with the mean for the entire column or 0 for the rest.\n",
    "combined = combined.fillna(combined.mean())\n",
    "combined = combined.fillna(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
